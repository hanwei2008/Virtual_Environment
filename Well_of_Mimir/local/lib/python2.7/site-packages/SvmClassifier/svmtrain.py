#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author: caiyuantao
# @Date:   2014-12-18
# @Email: 755864446@qq.com
# svmtrain.py
# Copyright (c) 2014 Chengdu Lanjing Data&Information Co., Ltd
import os
import sys
from pyspark import SparkContext
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import SVMWithSGD
from preprocess import pre_process
from feature import feature_selection, tf, idf, feature_vector
import cPickle as pickle
from util import read_file


def svm_train(sc, top_path, stopwords_dict=None):
    #   留个词词典接口，如果有新的词典，把词典放到该目录下
    curpath = os.path.normpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))
    if stopwords_dict is None:
        stopwords = set(read_file(os.path.join(curpath, u"stopwords.txt")).split())
    else:
        stopwords = set(read_file(os.path.join(curpath, u"stopwords_dict.txt")).split())

    #   形成两类的文件夹的每个文本分词，去停用词，词频统计结果{'pos':[counter,..],'neg':[counter]}

    sub_folder = os.listdir(top_path)
    if len(sub_folder) != 2:
        raise OSError("need and only need two folder")

    top_folder_dict = {}
    for name in sub_folder:
        top_folder_dict[name] = pre_process(os.path.join(top_path, name), stopwords)

    #   选出两类直接区分度最大的词作为这两类的特征词集
    topk = 500
    features = feature_selection(top_folder_dict[sub_folder[1]], top_folder_dict[sub_folder[0]], topk)

    #   计算两类的IDF
    IDF = idf(top_folder_dict[sub_folder[1]], top_folder_dict[sub_folder[1]], features)

    #   每一类每一篇文本在指定二分类下的向量表示[(),()...]
    vector1 = {'1.0': feature_vector(tf(top_folder_dict[sub_folder[1]], features), IDF)}
    vector0 = {'0.0': feature_vector(tf(top_folder_dict[sub_folder[0]], features), IDF)}

    #   转为Spark所需要的输入格式[Labpoint(0.0,[]),...]
    labpoint1 = [LabeledPoint(1.0, list) for list in vector1['1.0']]
    labpoint0 = [LabeledPoint(0.0, list) for list in vector0['0.0']]
    train_data = labpoint1 + labpoint0

    classifier = SVMWithSGD.train(sc.parallelize(train_data))

    path = os.path.join(curpath, 'svm_' + sub_folder[1] + '_' + sub_folder[0] + '.pkl')
    if os.path.isfile(path): os.remove(path)

    with open(path, 'wb') as output:
        pickle.dump((features, IDF, classifier), output)

if __name__ == "__main__":

    if len(sys.argv) < 2:
        print >> sys.stderr, "Usage: svmtrain <folder_name> <stopwords_dict>"
        exit(1)

    if len(sys.argv) == 2:
        stopwords_dict = None
    else:
        stopwords_dict = sys.argv[2]
    sc = SparkContext("local", "svmtrain.py")
    svm_train(sc, sys.argv[1], stopwords_dict)
    sc.stop()
