#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author: caiyuantao
# @Date:   2014-12-17
# @Email: 755864446@qq.com
# preprocess.py
# Copyright (c) 2014 Chengdu Lanjing Data&Information Co., Ltd
import os
from util import read_file
import re
import jieba
import collections


'''
预处理部分： 分词，去停用词，统计词频
'''


# 指定文档的分句(待写)
# 指定文本的分词
def word_segment(content):
    # jieba.enable_parallel(3)
    chinese = re.sub(ur"[^\u4E00-\u9FFF]+", u"", content)       # 提取中文
    return jieba.cut(chinese)   # 用jieba分词器进行分词generator


# 指定文本分完词后去停用词
def word_filter(words, stopwords):
    return (w for w in words if w not in stopwords)
    # 返回一个generator


# 统计词频
def word_counter(words):
    return collections.Counter(words)
    # 返回一个Counter({词：词频})


# 预处理一个文件夹和文本
def pre_process(path, stopwords):
    if os.path.isdir(path):
        folder_list = []
        for file_name in os.listdir(path):
            file_path = os.path.join(path, file_name)
            if os.path.isfile(file_path):
                folder_content = read_file(file_path)
                folder_words = word_segment(folder_content)
                words_filted = word_filter(folder_words, stopwords)
                folder_list.append(word_counter(words_filted))
        if len(folder_list) == 0:
            raise OSError('The folder has not file that we needed')
        return folder_list  # 形成一个文件夹里面每一个文本分词的结果[Counter({词：词频},),]
    else:
        if os.path.isfile(path):
            file_content = read_file(path)
            file_words = word_segment(file_content)
            words_filted = word_filter(file_words, stopwords)
            file_list = word_counter(words_filted)
        else:
            raise OSError('It is not the file that we needed')
        return file_list
